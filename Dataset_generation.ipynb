{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e70fc182",
   "metadata": {
    "cellId": "b3qpmo3e7ilp8aqd6uh6n"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import pyloudnorm as pyln\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8f546bce",
   "metadata": {
    "cellId": "cjpa7hzht65owy74a56xua"
   },
   "outputs": [],
   "source": [
    "def snr_mixer(clean, noise, snr):\n",
    "    amp_noise = np.linalg.norm(clean) / 10**(snr / 20)\n",
    "    noise_norm = (noise / np.linalg.norm(noise)) * amp_noise\n",
    "    mix = clean + noise_norm\n",
    "    return mix\n",
    "\n",
    "def vad_merge(w, top_db):\n",
    "    intervals = librosa.effects.split(w, top_db=top_db)\n",
    "    temp = list()\n",
    "    for s, e in intervals:\n",
    "        temp.append(w[s:e])\n",
    "    return np.concatenate(temp, axis=None)\n",
    "\n",
    "def cut_audios(s1, s2, sec, sr):\n",
    "    cut_len = sr * sec\n",
    "    len1 = len(s1)\n",
    "    len2 = len(s2)\n",
    "    s1_cut = []\n",
    "    s2_cut = []\n",
    "    segment = 0\n",
    "    while (segment + 1) * cut_len < len1 and (segment + 1) * cut_len < len2:\n",
    "        s1_cut.append(s1[segment * cut_len:(segment + 1) * cut_len])\n",
    "        s2_cut.append(s2[segment * cut_len:(segment + 1) * cut_len])\n",
    "        segment += 1\n",
    "    return s1_cut, s2_cut\n",
    "\n",
    "def fix_length(s1, s2, min_or_max='max'):\n",
    "    # Fix length\n",
    "    if min_or_max == 'min':\n",
    "        utt_len = np.minimum(len(s1), len(s2))\n",
    "        s1 = s1[:utt_len]\n",
    "        s2 = s2[:utt_len]\n",
    "    else:  # max\n",
    "        utt_len = np.maximum(len(s1), len(s2))\n",
    "        s1 = np.append(s1, np.zeros(utt_len - len(s1)))\n",
    "        s2 = np.append(s2, np.zeros(utt_len - len(s2)))\n",
    "    return s1, s2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "792ccd93",
   "metadata": {
    "cellId": "h97vru5zh3wppmqv39jno"
   },
   "outputs": [],
   "source": [
    "def create_mix(idx, triplet, snr_levels, out_dir, test=False, sr=16000, **kwargs):\n",
    "    trim_db, vad_db = kwargs[\"trim_db\"], kwargs[\"vad_db\"]\n",
    "    audioLen = kwargs[\"audioLen\"]\n",
    "\n",
    "    s1_path = triplet[\"target\"]\n",
    "    s2_path = triplet[\"noise\"]\n",
    "    ref_path = triplet[\"reference\"]\n",
    "    target_id = triplet[\"target_id\"]\n",
    "    noise_id = triplet[\"noise_id\"]\n",
    "\n",
    "    s1, _ = sf.read(os.path.join('', s1_path))\n",
    "    s2, _ = sf.read(os.path.join('', s2_path))\n",
    "    ref, _ = sf.read(os.path.join('', ref_path))\n",
    "\n",
    "    meter = pyln.Meter(sr) # create BS.1770 meter\n",
    "\n",
    "    louds1 = meter.integrated_loudness(s1)\n",
    "    louds2 = meter.integrated_loudness(s2)\n",
    "    loudsRef = meter.integrated_loudness(ref)\n",
    "\n",
    "    s1Norm = pyln.normalize.loudness(s1, louds1, -29)\n",
    "    s2Norm = pyln.normalize.loudness(s2, louds2, -29)\n",
    "    refNorm = pyln.normalize.loudness(ref, loudsRef, -23.0)\n",
    "\n",
    "    amp_s1 = np.max(np.abs(s1Norm))\n",
    "    amp_s2 = np.max(np.abs(s2Norm))\n",
    "    amp_ref = np.max(np.abs(refNorm))\n",
    "\n",
    "    if amp_s1 == 0 or amp_s2 == 0 or amp_ref == 0:\n",
    "        return\n",
    "\n",
    "    if trim_db:\n",
    "        ref, _ = librosa.effects.trim(refNorm, top_db=trim_db)\n",
    "        s1, _ = librosa.effects.trim(s1Norm, top_db=trim_db)\n",
    "        s2, _ = librosa.effects.trim(s2Norm, top_db=trim_db)\n",
    "\n",
    "    if len(ref) < sr:\n",
    "        return\n",
    "\n",
    "    path_mix = os.path.join(out_dir, f\"{target_id}_{noise_id}_\" + \"%06d\" % idx + \"-mixed.wav\")\n",
    "    path_target = os.path.join(out_dir, f\"{target_id}_{noise_id}_\" + \"%06d\" % idx + \"-target.wav\")\n",
    "    path_ref = os.path.join(out_dir, f\"{target_id}_{noise_id}_\" + \"%06d\" % idx + \"-ref.wav\")\n",
    "\n",
    "    snr = np.random.choice(snr_levels, 1).item()\n",
    "\n",
    "    if not test:\n",
    "        s1, s2 = vad_merge(s1, vad_db), vad_merge(s2, vad_db)\n",
    "        s1_cut, s2_cut = cut_audios(s1, s2, audioLen, sr)\n",
    "\n",
    "        for i in range(len(s1_cut)):\n",
    "            mix = snr_mixer(s1_cut[i], s2_cut[i], snr)\n",
    "\n",
    "            louds1 = meter.integrated_loudness(s1_cut[i])\n",
    "            s1_cut[i] = pyln.normalize.loudness(s1_cut[i], louds1, -23.0)\n",
    "            loudMix = meter.integrated_loudness(mix)\n",
    "            mix = pyln.normalize.loudness(mix, loudMix, -23.0)\n",
    "\n",
    "            path_mix_i = path_mix.replace(\"-mixed.wav\", f\"_{i}-mixed.wav\")\n",
    "            path_target_i = path_target.replace(\"-target.wav\", f\"_{i}-target.wav\")\n",
    "            path_ref_i = path_ref.replace(\"-ref.wav\", f\"_{i}-ref.wav\")\n",
    "            sf.write(path_mix_i, mix, sr)\n",
    "            sf.write(path_target_i, s1_cut[i], sr)\n",
    "            sf.write(path_ref_i, ref, sr)\n",
    "    else:\n",
    "        s1, s2 = fix_length(s1, s2, 'max')\n",
    "        mix = snr_mixer(s1, s2, snr)\n",
    "        louds1 = meter.integrated_loudness(s1)\n",
    "        s1 = pyln.normalize.loudness(s1, louds1, -23.0)\n",
    "\n",
    "        loudMix = meter.integrated_loudness(mix)\n",
    "        mix = pyln.normalize.loudness(mix, loudMix, -23.0)\n",
    "\n",
    "        sf.write(path_mix, mix, sr)\n",
    "        sf.write(path_target, s1, sr)\n",
    "        sf.write(path_ref, ref, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d6e52f97",
   "metadata": {
    "cellId": "pmo472bf1chtfej9ij5ny"
   },
   "outputs": [],
   "source": [
    "class LibriSpeechSpeakerFiles:\n",
    "    def __init__(self, speaker_id, audios_dir, audioTemplate=\"*-norm.wav\"):\n",
    "        self.id = speaker_id\n",
    "        self.files = []\n",
    "        self.audioTemplate=audioTemplate\n",
    "        self.files = self.find_files_by_worker(audios_dir)\n",
    "\n",
    "    def find_files_by_worker(self, audios_dir):\n",
    "        speakerDir = os.path.join(audios_dir,self.id) #it is a string\n",
    "        chapterDirs = os.scandir(speakerDir)\n",
    "        files=[]\n",
    "        for chapterDir in chapterDirs:\n",
    "            files = files + [file for file in glob(os.path.join(speakerDir,chapterDir.name)+\"/\"+self.audioTemplate)]\n",
    "        return files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ae3338bf",
   "metadata": {
    "cellId": "ju67q1gkvqjgzeo2ahqx2i"
   },
   "outputs": [],
   "source": [
    "class MixtureGenerator:\n",
    "    def __init__(self, speakers_files, out_folder, nfiles=5000, test=False, randomState=42):\n",
    "        self.speakers_files = speakers_files # list of SpeakerFiles for every speaker_id\n",
    "        self.nfiles = nfiles\n",
    "        self.randomState = randomState\n",
    "        self.out_folder = out_folder\n",
    "        self.test = test\n",
    "        random.seed(self.randomState)\n",
    "        if not os.path.exists(self.out_folder):\n",
    "            os.makedirs(self.out_folder)\n",
    "\n",
    "    def generate_triplets(self):\n",
    "        i = 0\n",
    "        all_triplets = {\"reference\": [], \"target\": [], \"noise\": [], \"target_id\": [], \"noise_id\": []}\n",
    "        while i < self.nfiles:\n",
    "            spk1, spk2 = random.sample(self.speakers_files, 2)\n",
    "\n",
    "            if len(spk1.files) < 2 or len(spk2.files) < 2:\n",
    "                continue\n",
    "\n",
    "            target, reference = random.sample(spk1.files, 2)\n",
    "            noise = random.choice(spk2.files)\n",
    "            all_triplets[\"reference\"].append(reference)\n",
    "            all_triplets[\"target\"].append(target)\n",
    "            all_triplets[\"noise\"].append(noise)\n",
    "            all_triplets[\"target_id\"].append(spk1.id)\n",
    "            all_triplets[\"noise_id\"].append(spk2.id)\n",
    "            i += 1\n",
    "\n",
    "        return all_triplets\n",
    "\n",
    "    def triplet_generator(self, target_speaker, noise_speaker, number_of_triplets):\n",
    "        max_num_triplets = min(len(target_speaker.files), len(noise_speaker.files))\n",
    "        number_of_triplets = min(max_num_triplets, number_of_triplets)\n",
    "\n",
    "        target_samples = random.sample(target_speaker.files, k=number_of_triplets)\n",
    "        reference_samples = random.sample(target_speaker.files, k=number_of_triplets)\n",
    "        noise_samples = random.sample(noise_speaker.files, k=number_of_triplets)\n",
    "\n",
    "        triplets = {\"reference\": [], \"target\": [], \"noise\": [],\n",
    "                    \"target_id\": [target_speaker.id] * number_of_triplets, \"noise_id\": [noise_speaker.id] * number_of_triplets}\n",
    "        triplets[\"target\"] += target_samples\n",
    "        triplets[\"reference\"] += reference_samples\n",
    "        triplets[\"noise\"] += noise_samples\n",
    "\n",
    "        return triplets\n",
    "\n",
    "    def generate_mixes(self, snr_levels=[0], num_workers=10, update_steps=10, **kwargs):\n",
    "\n",
    "        triplets = self.generate_triplets()\n",
    "\n",
    "        with ProcessPoolExecutor(max_workers=num_workers) as pool:\n",
    "            futures = []\n",
    "\n",
    "            for i in range(self.nfiles):\n",
    "                triplet = {\"reference\": triplets[\"reference\"][i],\n",
    "                           \"target\": triplets[\"target\"][i],\n",
    "                           \"noise\": triplets[\"noise\"][i],\n",
    "                           \"target_id\": triplets[\"target_id\"][i],\n",
    "                           \"noise_id\": triplets[\"noise_id\"][i]}\n",
    "\n",
    "                futures.append(pool.submit(create_mix, i, triplet,\n",
    "                                           snr_levels, self.out_folder,\n",
    "                                           test=self.test, **kwargs))\n",
    "\n",
    "            for i, future in enumerate(futures):\n",
    "                future.result()\n",
    "                if (i + 1) % max(self.nfiles // update_steps, 1) == 0:\n",
    "                    print(f\"Files Processed | {i + 1} out of {self.nfiles}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e3f4a284",
   "metadata": {
    "cellId": "dv2n8wlen5b3jnq6ibifk9"
   },
   "outputs": [],
   "source": [
    "path_train = # path to folder with audio to sample train examples from\n",
    "path_val =  # path to folder with audio to sample validation examples from\n",
    "\n",
    "path_mixtures_train = # path to folder where to place train split of mixed dataset\n",
    "path_mixtures_val =  # path to folder where to place validation split of mixed dataset\n",
    "\n",
    "speakersTrain = [el.name for el in os.scandir(path_train)]\n",
    "speakersVal = [el.name for el in os.scandir(path_val)]\n",
    "\n",
    "speakers_files_train = [LibriSpeechSpeakerFiles(i, path_train, audioTemplate=\"*.flac\") for i in speakersTrain]\n",
    "speakers_files_val = [LibriSpeechSpeakerFiles(i, path_val, audioTemplate=\"*.flac\") for i in speakersVal]\n",
    "\n",
    "mixer_train = MixtureGenerator(speakers_files_train,\n",
    "                                path_mixtures_train,\n",
    "                                nfiles=10000,\n",
    "                                test=False)\n",
    "\n",
    "mixer_val = MixtureGenerator(speakers_files_val,\n",
    "                                path_mixtures_val,\n",
    "                                nfiles=1000,\n",
    "                                test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7d92c373",
   "metadata": {
    "cellId": "yb1se1d7gemh49hgu7m9e",
    "execution_id": "713a6fce-27fc-4579-a5cc-407d1f0ffca0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files Processed | 100 out of 10000\n",
      "Files Processed | 200 out of 10000\n",
      "Files Processed | 300 out of 10000\n",
      "Files Processed | 400 out of 10000\n",
      "Files Processed | 500 out of 10000\n",
      "Files Processed | 600 out of 10000\n",
      "Files Processed | 700 out of 10000\n",
      "Files Processed | 800 out of 10000\n",
      "Files Processed | 900 out of 10000\n",
      "Files Processed | 1000 out of 10000\n",
      "Files Processed | 1100 out of 10000\n",
      "Files Processed | 1200 out of 10000\n",
      "Files Processed | 1300 out of 10000\n",
      "Files Processed | 1400 out of 10000\n",
      "Files Processed | 1500 out of 10000\n",
      "Files Processed | 1600 out of 10000\n",
      "Files Processed | 1700 out of 10000\n",
      "Files Processed | 1800 out of 10000\n",
      "Files Processed | 1900 out of 10000\n",
      "Files Processed | 2000 out of 10000\n",
      "Files Processed | 2100 out of 10000\n",
      "Files Processed | 2200 out of 10000\n",
      "Files Processed | 2300 out of 10000\n",
      "Files Processed | 2400 out of 10000\n",
      "Files Processed | 2500 out of 10000\n",
      "Files Processed | 2600 out of 10000\n",
      "Files Processed | 2700 out of 10000\n",
      "Files Processed | 2800 out of 10000\n",
      "Files Processed | 2900 out of 10000\n",
      "Files Processed | 3000 out of 10000\n",
      "Files Processed | 3100 out of 10000\n",
      "Files Processed | 3200 out of 10000\n",
      "Files Processed | 3300 out of 10000\n",
      "Files Processed | 3400 out of 10000\n",
      "Files Processed | 3500 out of 10000\n",
      "Files Processed | 3600 out of 10000\n",
      "Files Processed | 3700 out of 10000\n",
      "Files Processed | 3800 out of 10000\n",
      "Files Processed | 3900 out of 10000\n",
      "Files Processed | 4000 out of 10000\n",
      "Files Processed | 4100 out of 10000\n",
      "Files Processed | 4200 out of 10000\n",
      "Files Processed | 4300 out of 10000\n",
      "Files Processed | 4400 out of 10000\n",
      "Files Processed | 4500 out of 10000\n",
      "Files Processed | 4600 out of 10000\n",
      "Files Processed | 4700 out of 10000\n",
      "Files Processed | 4800 out of 10000\n",
      "Files Processed | 4900 out of 10000\n",
      "Files Processed | 5000 out of 10000\n",
      "Files Processed | 5100 out of 10000\n",
      "Files Processed | 5200 out of 10000\n",
      "Files Processed | 5300 out of 10000\n",
      "Files Processed | 5400 out of 10000\n",
      "Files Processed | 5500 out of 10000\n",
      "Files Processed | 5600 out of 10000\n",
      "Files Processed | 5700 out of 10000\n",
      "Files Processed | 5800 out of 10000\n",
      "Files Processed | 5900 out of 10000\n",
      "Files Processed | 6000 out of 10000\n",
      "Files Processed | 6100 out of 10000\n",
      "Files Processed | 6200 out of 10000\n",
      "Files Processed | 6300 out of 10000\n",
      "Files Processed | 6400 out of 10000\n",
      "Files Processed | 6500 out of 10000\n",
      "Files Processed | 6600 out of 10000\n",
      "Files Processed | 6700 out of 10000\n",
      "Files Processed | 6800 out of 10000\n",
      "Files Processed | 6900 out of 10000\n",
      "Files Processed | 7000 out of 10000\n",
      "Files Processed | 7100 out of 10000\n",
      "Files Processed | 7200 out of 10000\n",
      "Files Processed | 7300 out of 10000\n",
      "Files Processed | 7400 out of 10000\n",
      "Files Processed | 7500 out of 10000\n",
      "Files Processed | 7600 out of 10000\n",
      "Files Processed | 7700 out of 10000\n",
      "Files Processed | 7800 out of 10000\n",
      "Files Processed | 7900 out of 10000\n",
      "Files Processed | 8000 out of 10000\n",
      "Files Processed | 8100 out of 10000\n",
      "Files Processed | 8200 out of 10000\n",
      "Files Processed | 8300 out of 10000\n",
      "Files Processed | 8400 out of 10000\n",
      "Files Processed | 8500 out of 10000\n",
      "Files Processed | 8600 out of 10000\n",
      "Files Processed | 8700 out of 10000\n",
      "Files Processed | 8800 out of 10000\n",
      "Files Processed | 8900 out of 10000\n",
      "Files Processed | 9000 out of 10000\n",
      "Files Processed | 9100 out of 10000\n",
      "Files Processed | 9200 out of 10000\n",
      "Files Processed | 9300 out of 10000\n",
      "Files Processed | 9400 out of 10000\n",
      "Files Processed | 9500 out of 10000\n",
      "Files Processed | 9600 out of 10000\n",
      "Files Processed | 9700 out of 10000\n",
      "Files Processed | 9800 out of 10000\n",
      "Files Processed | 9900 out of 10000\n",
      "Files Processed | 10000 out of 10000\n",
      "Files Processed | 10 out of 1000\n",
      "Files Processed | 20 out of 1000\n",
      "Files Processed | 30 out of 1000\n",
      "Files Processed | 40 out of 1000\n",
      "Files Processed | 50 out of 1000\n",
      "Files Processed | 60 out of 1000\n",
      "Files Processed | 70 out of 1000\n",
      "Files Processed | 80 out of 1000\n",
      "Files Processed | 90 out of 1000\n",
      "Files Processed | 100 out of 1000\n",
      "Files Processed | 110 out of 1000\n",
      "Files Processed | 120 out of 1000\n",
      "Files Processed | 130 out of 1000\n",
      "Files Processed | 140 out of 1000\n",
      "Files Processed | 150 out of 1000\n",
      "Files Processed | 160 out of 1000\n",
      "Files Processed | 170 out of 1000\n",
      "Files Processed | 180 out of 1000\n",
      "Files Processed | 190 out of 1000\n",
      "Files Processed | 200 out of 1000\n",
      "Files Processed | 210 out of 1000\n",
      "Files Processed | 220 out of 1000\n",
      "Files Processed | 230 out of 1000\n",
      "Files Processed | 240 out of 1000\n",
      "Files Processed | 250 out of 1000\n",
      "Files Processed | 260 out of 1000\n",
      "Files Processed | 270 out of 1000\n",
      "Files Processed | 280 out of 1000\n",
      "Files Processed | 290 out of 1000\n",
      "Files Processed | 300 out of 1000\n",
      "Files Processed | 310 out of 1000\n",
      "Files Processed | 320 out of 1000\n",
      "Files Processed | 330 out of 1000\n",
      "Files Processed | 340 out of 1000\n",
      "Files Processed | 350 out of 1000\n",
      "Files Processed | 360 out of 1000\n",
      "Files Processed | 370 out of 1000\n",
      "Files Processed | 380 out of 1000\n",
      "Files Processed | 390 out of 1000\n",
      "Files Processed | 400 out of 1000\n",
      "Files Processed | 410 out of 1000\n",
      "Files Processed | 420 out of 1000\n",
      "Files Processed | 430 out of 1000\n",
      "Files Processed | 440 out of 1000\n",
      "Files Processed | 450 out of 1000\n",
      "Files Processed | 460 out of 1000\n",
      "Files Processed | 470 out of 1000\n",
      "Files Processed | 480 out of 1000\n",
      "Files Processed | 490 out of 1000\n",
      "Files Processed | 500 out of 1000\n",
      "Files Processed | 510 out of 1000\n",
      "Files Processed | 520 out of 1000\n",
      "Files Processed | 530 out of 1000\n",
      "Files Processed | 540 out of 1000\n",
      "Files Processed | 550 out of 1000\n",
      "Files Processed | 560 out of 1000\n",
      "Files Processed | 570 out of 1000\n",
      "Files Processed | 580 out of 1000\n",
      "Files Processed | 590 out of 1000\n",
      "Files Processed | 600 out of 1000\n",
      "Files Processed | 610 out of 1000\n",
      "Files Processed | 620 out of 1000\n",
      "Files Processed | 630 out of 1000\n",
      "Files Processed | 640 out of 1000\n",
      "Files Processed | 650 out of 1000\n",
      "Files Processed | 660 out of 1000\n",
      "Files Processed | 670 out of 1000\n",
      "Files Processed | 680 out of 1000\n",
      "Files Processed | 690 out of 1000\n",
      "Files Processed | 700 out of 1000\n",
      "Files Processed | 710 out of 1000\n",
      "Files Processed | 720 out of 1000\n",
      "Files Processed | 730 out of 1000\n",
      "Files Processed | 740 out of 1000\n",
      "Files Processed | 750 out of 1000\n",
      "Files Processed | 760 out of 1000\n",
      "Files Processed | 770 out of 1000\n",
      "Files Processed | 780 out of 1000\n",
      "Files Processed | 790 out of 1000\n",
      "Files Processed | 800 out of 1000\n",
      "Files Processed | 810 out of 1000\n",
      "Files Processed | 820 out of 1000\n",
      "Files Processed | 830 out of 1000\n",
      "Files Processed | 840 out of 1000\n",
      "Files Processed | 850 out of 1000\n",
      "Files Processed | 860 out of 1000\n",
      "Files Processed | 870 out of 1000\n",
      "Files Processed | 880 out of 1000\n",
      "Files Processed | 890 out of 1000\n",
      "Files Processed | 900 out of 1000\n",
      "Files Processed | 910 out of 1000\n",
      "Files Processed | 920 out of 1000\n",
      "Files Processed | 930 out of 1000\n",
      "Files Processed | 940 out of 1000\n",
      "Files Processed | 950 out of 1000\n",
      "Files Processed | 960 out of 1000\n",
      "Files Processed | 970 out of 1000\n",
      "Files Processed | 980 out of 1000\n",
      "Files Processed | 990 out of 1000\n",
      "Files Processed | 1000 out of 1000\n"
     ]
    }
   ],
   "source": [
    "mixer_train.generate_mixes(snr_levels=[0, 5],\n",
    "                           num_workers=2,\n",
    "                           update_steps=100,\n",
    "                           trim_db=20,\n",
    "                           vad_db=20,\n",
    "                           audioLen=3)\n",
    "\n",
    "mixer_val.generate_mixes(snr_levels=[0],\n",
    "                           num_workers=2,\n",
    "                           update_steps=100,\n",
    "                           trim_db=None,\n",
    "                           vad_db=20,\n",
    "                           audioLen=3)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "189f6bd4-5717-4d0f-8dda-ba0b70072e34",
  "notebookPath": "Dataset_generation.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
